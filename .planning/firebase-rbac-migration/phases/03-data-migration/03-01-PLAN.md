---
phase: 03-data-migration
type: execute
---

<objective>
Create an idempotent data migration script to transfer mock_db.json data to production Firestore using BulkWriter for efficient batch operations.

Purpose: Migrate existing test data to production Firestore so the application isn't empty after migration. The script must be idempotent (can run multiple times without duplicates) and handle errors gracefully.
Output: tools/seed_firestore.py script with BulkWriter, idempotency checks, progress logging, and rollback capability.
</objective>

<execution_context>
@~/.Opencode/skills/create-plans/workflows/execute-phase.md
@~/.Opencode/skills/create-plans/templates/summary.md
</execution_context>

<context>
@.planning/firebase-rbac-migration/BRIEF.md
@.planning/firebase-rbac-migration/ROADMAP.md
@FIREBASE_RBAC_MIGRATION_PLAN.md
@mock_db.json
@documentations/firebase-schema.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Analyze mock_db.json structure</name>
  <files>mock_db.json</files>
  <action>
    Read and analyze mock_db.json to understand the data structure.
    
    Document the structure:
    - What collections exist?
    - What fields does each document have?
    - What are the relationships between collections?
    - Which data needs to be migrated first (users)?
    - Are there any nested collections (semesters under departments)?
    
    Create a mapping document showing:
    - Source: mock_db.json structure
    - Destination: Firestore collection structure
    - Transformations needed (field renames, type conversions)
    - Dependencies (users must be created before notes, etc.)
    
    Example mapping:
    ```
    Source (mock_db.json):
      - departments (root object)
        - {deptId}: Department data
          - semesters (nested)
            - {semId}: Semester data
    
    Destination (Firestore):
      - departments/{deptId} - Department document
      - semesters/{semId} - Semester document (with departmentId field)
    ```
    
    Note: Document any field name changes needed (e.g., camelCase to snake_case).
  </action>
  <verify>
    1. Read mock_db.json and understand its structure
    2. Document the collection hierarchy
    3. List all fields for each entity type
  </verify>
  <done>mock_db.json structure analyzed and documented with mapping to Firestore</done>
</task>

<task type="auto">
  <name>Task 2: Create migration configuration</name>
  <files>tools/migration_config.py</files>
  <action>
    Create a configuration file for the migration with settings and mappings.
    
    ```python
    """
    ============================================================================
    FILE: migration_config.py
    LOCATION: tools/migration_config.py
    ============================================================================

    PURPOSE:
        Configuration and mappings for Firestore data migration

    ROLE IN PROJECT:
        Defines source-to-destination mappings, field transformations,
        and migration settings

    DEPENDENCIES:
        - None (pure configuration)
    ============================================================================
    """

    from typing import Dict, List, Callable
    from datetime import datetime

    # Migration settings
    MIGRATION_SETTINGS = {
        'batch_size': 500,  # Number of documents per batch
        'max_retries': 3,   # Max retry attempts per document
        'rate_limit': 500,  # Writes per second (Firestore limit)
        'schema_version': 1,  # Migration schema version
    }

    # Collection migration order (dependencies first)
    COLLECTION_ORDER = [
        'users',           # No dependencies
        'departments',     # No dependencies
        'semesters',       # May reference departments
        'subjects',        # References departments/semesters
        'modules',         # References subjects
        'notes',           # References subjects and users
    ]

    # Field mappings for transformations
    # Key: source field name, Value: destination field name or transformation function
    FIELD_MAPPINGS = {
        'users': {
            'id': 'uid',  # Rename
            'name': 'displayName',  # Rename
            'department_id': 'departmentId',  # snake_case to camelCase
            'subject_ids': 'subjectIds',
            # Add timestamps
            '_add': {
                'createdAt': lambda: datetime.utcnow(),
                'updatedAt': lambda: datetime.utcnow(),
                '_v': 1,  # Schema version for future migrations
            }
        },
        'departments': {
            'id': lambda doc: doc['id'],  # Keep as document ID, not field
            '_add': {
                'createdAt': lambda: datetime.utcnow(),
                'updatedAt': lambda: datetime.utcnow(),
                '_v': 1,
            }
        },
        # Add mappings for other collections...
    }

    # Required fields validation
    REQUIRED_FIELDS = {
        'users': ['uid', 'email', 'role', 'status'],
        'departments': ['name'],
        'subjects': ['name', 'departmentId'],
        'notes': ['title', 'content', 'subjectId'],
    }

    # Default values for optional fields
    DEFAULT_VALUES = {
        'users': {
            'status': 'active',
            'subjectIds': [],
            'departmentId': None,
        },
        'notes': {
            'createdAt': lambda: datetime.utcnow(),
        }
    }

    def get_field_mapping(collection: str, field: str) -> str:
        """Get destination field name for a source field."""
        mapping = FIELD_MAPPINGS.get(collection, {})
        return mapping.get(field, field)

    def get_required_fields(collection: str) -> List[str]:
        """Get required fields for a collection."""
        return REQUIRED_FIELDS.get(collection, [])

    def get_default_values(collection: str) -> Dict:
        """Get default values for a collection."""
        defaults = DEFAULT_VALUES.get(collection, {})
        # Evaluate callable defaults
        return {
            k: v() if callable(v) else v
            for k, v in defaults.items()
        }
    ```
    
    Customize the mappings based on actual mock_db.json structure.
  </action>
  <verify>
    1. Check tools/migration_config.py exists
    2. Verify COLLECTION_ORDER has dependencies first
    3. Verify FIELD_MAPPINGS match mock_db.json structure
  </verify>
  <done>Migration configuration created with field mappings and settings</done>
</task>

<task type="auto">
  <name>Task 3: Create idempotent migration script with BulkWriter</name>
  <files>tools/seed_firestore.py</files>
  <action>
    Create the main migration script using BulkWriter for efficient batch operations.
    
    ```python
    """
    ============================================================================
    FILE: seed_firestore.py
    LOCATION: tools/seed_firestore.py
    ============================================================================

    PURPOSE:
        Migrate data from mock_db.json to Firestore with idempotency support

    ROLE IN PROJECT:
        One-time migration script for initial data seeding
        Can be re-run safely (idempotent) to update existing documents

    DEPENDENCIES:
        - firebase-admin
        - Python 3.10+
        - serviceAccountKey.json (Firebase credentials)
        - mock_db.json (source data)

    USAGE:
        python tools/seed_firestore.py
        
        # With options:
        python tools/seed_firestore.py --dry-run
        python tools/seed_firestore.py --collection users
        python tools/seed_firestore.py --reset
    ============================================================================
    """

    import json
    import argparse
    import logging
    from datetime import datetime
    from typing import Dict, List, Any
    from pathlib import Path

    import firebase_admin
    from firebase_admin import credentials, firestore
    from google.cloud.firestore import BulkWriter

    from migration_config import (
        MIGRATION_SETTINGS,
        COLLECTION_ORDER,
        FIELD_MAPPINGS,
        get_required_fields,
        get_default_values,
    )

    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )
    logger = logging.getLogger(__name__)

    # Paths
    PROJECT_ROOT = Path(__file__).parent.parent
    MOCK_DB_PATH = PROJECT_ROOT / 'mock_db.json'
    SERVICE_ACCOUNT_PATH = PROJECT_ROOT / 'serviceAccountKey.json'


    class FirestoreMigrator:
        """Handles migration from mock_db.json to Firestore."""
        
        def __init__(self, dry_run: bool = False):
            self.dry_run = dry_run
            self.db = None
            self.stats = {
                'created': 0,
                'updated': 0,
                'errors': 0,
                'skipped': 0,
            }
        
        def initialize_firebase(self):
            """Initialize Firebase Admin SDK."""
            if not SERVICE_ACCOUNT_PATH.exists():
                raise FileNotFoundError(
                    f"Service account key not found: {SERVICE_ACCOUNT_PATH}\n"
                    "Please download it from Firebase Console."
                )
            
            if not firebase_admin._apps:
                cred = credentials.Certificate(str(SERVICE_ACCOUNT_PATH))
                firebase_admin.initialize_app(cred)
            
            self.db = firestore.client()
            logger.info("Firebase initialized successfully")
        
        def load_mock_data(self) -> Dict[str, Any]:
            """Load data from mock_db.json."""
            if not MOCK_DB_PATH.exists():
                raise FileNotFoundError(f"mock_db.json not found: {MOCK_DB_PATH}")
            
            with open(MOCK_DB_PATH, 'r') as f:
                data = json.load(f)
            
            logger.info(f"Loaded mock data with {len(data)} top-level keys")
            return data
        
        def transform_document(
            self,
            collection: str,
            doc_id: str,
            data: Dict[str, Any]
        ) -> Dict[str, Any]:
            """Transform document to match Firestore schema."""
            transformed = {'_v': MIGRATION_SETTINGS['schema_version']}
            
            # Get field mappings for this collection
            mappings = FIELD_MAPPINGS.get(collection, {})
            
            # Apply field mappings
            for source_field, value in data.items():
                if source_field in mappings:
                    mapping = mappings[source_field]
                    if callable(mapping):
                        # Transformation function
                        dest_field = source_field
                        transformed[dest_field] = mapping(data)
                    elif isinstance(mapping, str):
                        # Simple rename
                        dest_field = mapping
                        transformed[dest_field] = value
                    else:
                        dest_field = source_field
                        transformed[dest_field] = value
                else:
                    # No mapping, keep as-is
                    transformed[source_field] = value
            
            # Add computed fields
            if '_add' in mappings:
                for field, value_or_func in mappings['_add'].items():
                    if callable(value_or_func):
                        transformed[field] = value_or_func()
                    else:
                        transformed[field] = value_or_func
            
            # Add defaults for missing optional fields
            defaults = get_default_values(collection)
            for field, value in defaults.items():
                if field not in transformed:
                    transformed[field] = value
            
            # Ensure required fields exist
            required = get_required_fields(collection)
            for field in required:
                if field not in transformed:
                    raise ValueError(
                        f"Required field '{field}' missing in {collection}/{doc_id}"
                    )
            
            return transformed
        
        def check_existing_document(
            self,
            collection: str,
            doc_id: str
        ) -> tuple[bool, Dict[str, Any] | None]:
            """Check if document exists and return its data."""
            doc_ref = self.db.collection(collection).document(doc_id)
            doc = doc_ref.get()
            
            if doc.exists:
                return True, doc.to_dict()
            return False, None
        
        def should_update(
            self,
            collection: str,
            existing_data: Dict[str, Any]
        ) -> bool:
            """Check if existing document should be updated."""
            # Check schema version
            existing_version = existing_data.get('_v', 0)
            current_version = MIGRATION_SETTINGS['schema_version']
            
            if existing_version < current_version:
                return True
            
            # Check if migration flag is set
            if existing_data.get('_migrated_at'):
                logger.debug(f"Skipping {collection} - already migrated")
                return False
            
            return True
        
        def migrate_collection(
            self,
            collection: str,
            documents: Dict[str, Any],
            bulk_writer: BulkWriter
        ):
            """Migrate all documents in a collection."""
            logger.info(f"Migrating {len(documents)} documents to {collection}")
            
            for doc_id, data in documents.items():
                try:
                    # Transform document
                    transformed = self.transform_document(collection, doc_id, data)
                    
                    # Check if document exists
                    exists, existing = self.check_existing_document(collection, doc_id)
                    
                    if exists and not self.should_update(collection, existing):
                        self.stats['skipped'] += 1
                        continue
                    
                    # Add migration metadata
                    transformed['_migrated_at'] = datetime.utcnow()
                    transformed['_migration_source'] = 'mock_db.json'
                    
                    if self.dry_run:
                        logger.info(f"[DRY RUN] Would write to {collection}/{doc_id}")
                        continue
                    
                    # Write document using BulkWriter
                    doc_ref = self.db.collection(collection).document(doc_id)
                    bulk_writer.set(doc_ref, transformed)
                    
                    if exists:
                        self.stats['updated'] += 1
                    else:
                        self.stats['created'] += 1
                    
                    logger.debug(f"Queued {collection}/{doc_id}")
                    
                except Exception as e:
                    logger.error(f"Error processing {collection}/{doc_id}: {e}")
                    self.stats['errors'] += 1
        
        def run(self, target_collection: str = None, reset: bool = False):
            """Run the migration."""
            logger.info("Starting Firestore migration")
            
            if self.dry_run:
                logger.info("DRY RUN MODE - No changes will be made")
            
            # Initialize Firebase
            self.initialize_firebase()
            
            # Load mock data
            mock_data = self.load_mock_data()
            
            # Handle reset option
            if reset and not self.dry_run:
                confirm = input(
                    "WARNING: This will DELETE all data in Firestore. "
                    "Type 'yes' to confirm: "
                )
                if confirm.lower() != 'yes':
                    logger.info("Reset cancelled")
                    return
                
                # Delete all collections
                for collection in COLLECTION_ORDER:
                    self._delete_collection(collection)
            
            # Create BulkWriter
            bulk_writer = self.db.bulk_writer()
            
            # Configure BulkWriter options
            bulk_writer._max_batch_size = MIGRATION_SETTINGS['batch_size']
            
            # Track errors
            def on_write_error(error):
                logger.error(f"Write failed: {error}")
                return error.failed_attempts < MIGRATION_SETTINGS['max_retries']
            
            bulk_writer.on_write_error(on_write_error)
            
            # Migrate collections in order
            for collection in COLLECTION_ORDER:
                if target_collection and collection != target_collection:
                    continue
                
                if collection not in mock_data:
                    logger.warning(f"Collection {collection} not found in mock data")
                    continue
                
                documents = mock_data[collection]
                if not isinstance(documents, dict):
                    logger.warning(f"Unexpected data type for {collection}")
                    continue
                
                self.migrate_collection(collection, documents, bulk_writer)
            
            # Flush BulkWriter
            if not self.dry_run:
                logger.info("Flushing writes to Firestore...")
                bulk_writer.close()
            
            # Print stats
            logger.info("=" * 50)
            logger.info("Migration complete!")
            logger.info(f"  Created: {self.stats['created']}")
            logger.info(f"  Updated: {self.stats['updated']}")
            logger.info(f"  Skipped: {self.stats['skipped']}")
            logger.info(f"  Errors:  {self.stats['errors']}")
            logger.info("=" * 50)
        
        def _delete_collection(self, collection: str):
            """Delete all documents in a collection."""
            logger.warning(f"Deleting all documents in {collection}")
            docs = self.db.collection(collection).stream()
            for doc in docs:
                doc.reference.delete()
                logger.debug(f"Deleted {collection}/{doc.id}")


    def main():
        parser = argparse.ArgumentParser(
            description='Migrate mock_db.json data to Firestore'
        )
        parser.add_argument(
            '--dry-run',
            action='store_true',
            help='Show what would be done without making changes'
        )
        parser.add_argument(
            '--collection',
            help='Migrate only a specific collection'
        )
        parser.add_argument(
            '--reset',
            action='store_true',
            help='Delete existing data before migration (DANGEROUS)'
        )
        
        args = parser.parse_args()
        
        migrator = FirestoreMigrator(dry_run=args.dry_run)
        migrator.run(
            target_collection=args.collection,
            reset=args.reset
        )


    if __name__ == '__main__':
        main()
    ```
    
    This script:
    - Uses BulkWriter for efficient batch operations
    - Checks schema version for idempotency
    - Supports dry-run mode
    - Tracks detailed statistics
    - Handles errors with retry logic
  </action>
  <verify>
    1. Check tools/seed_firestore.py exists
    2. Verify it imports from tools/migration_config.py
    3. Verify BulkWriter is used
    4. Verify idempotency logic (schema version, _migrated_at check)
  </verify>
  <done>Migration script created with BulkWriter, idempotency, and comprehensive logging</done>
</task>

<task type="auto">
  <name>Task 4: Create migration state tracking collection</name>
  <files>tools/seed_firestore.py (update)</files>
  <action>
    Enhance the migration script to track state in Firestore for better idempotency and recovery.
    
    Add a `_migrations` collection to track migration runs:
    
    ```python
    def record_migration_start(self) -> str:
        """Record migration start in Firestore."""
        if self.dry_run:
            return "dry-run-id"
        
        migration_ref = self.db.collection('_migrations').document()
        migration_data = {
            'started_at': datetime.utcnow(),
            'schema_version': MIGRATION_SETTINGS['schema_version'],
            'source': 'mock_db.json',
            'status': 'in_progress',
            'dry_run': self.dry_run,
        }
        migration_ref.set(migration_data)
        return migration_ref.id
    
    def record_migration_complete(self, migration_id: str):
        """Record migration completion."""
        if self.dry_run:
            return
        
        self.db.collection('_migrations').document(migration_id).update({
            'completed_at': datetime.utcnow(),
            'status': 'completed',
            'stats': self.stats,
        })
    
    def record_migration_error(self, migration_id: str, error: str):
        """Record migration error."""
        if self.dry_run:
            return
        
        self.db.collection('_migrations').document(migration_id).update({
            'failed_at': datetime.utcnow(),
            'status': 'failed',
            'error': error,
            'stats': self.stats,
        })
    ```
    
    Update the `run()` method to use these tracking functions:
    - Record start before migration
    - Record complete on success
    - Record error on failure
    
    Also add a resume capability:
    ```python
    def get_last_successful_migration(self) -> datetime | None:
        """Get timestamp of last successful migration."""
        docs = (
            self.db.collection('_migrations')
            .where('status', '==', 'completed')
            .order_by('completed_at', direction='DESCENDING')
            .limit(1)
            .stream()
        )
        
        for doc in docs:
            return doc.to_dict()['completed_at']
        return None
    ```
  </action>
  <verify>
    1. Update seed_firestore.py with migration tracking
    2. Verify _migrations collection is used
    3. Verify migration state is tracked (started, completed, failed)
  </verify>
  <done>Migration state tracking added for audit trail and recovery</done>
</task>

<task type="auto">
  <name>Task 5: Create backup script</name>
  <files>tools/backup_firestore.py</files>
  <action>
    Create a backup script using Firestore's managed export feature.
    
    ```python
    """
    ============================================================================
    FILE: backup_firestore.py
    LOCATION: tools/backup_firestore.py
    ============================================================================

    PURPOSE:
        Create a managed export backup of Firestore data

    ROLE IN PROJECT:
        Safety backup before running migrations
        Allows point-in-time recovery

    DEPENDENCIES:
        - Google Cloud SDK (gcloud)
        - Appropriate IAM permissions

    USAGE:
        python tools/backup_firestore.py
        python tools/backup_firestore.py --bucket gs://my-backup-bucket
    ============================================================================
    """

    import subprocess
    import argparse
    import logging
    from datetime import datetime
    from pathlib import Path

    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    def create_backup(bucket_name: str = None) -> str:
        """Create a Firestore backup using managed export."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if not bucket_name:
            # Default bucket: project-id-backups
            # You need to create this bucket first
            bucket_name = input(
                "Enter Cloud Storage bucket for backup (e.g., gs://my-project-backups): "
            )
        
        if not bucket_name.startswith('gs://'):
            bucket_name = f'gs://{bucket_name}'
        
        export_path = f"{bucket_name}/firestore-backup-{timestamp}"
        
        logger.info(f"Starting Firestore export to {export_path}")
        
        try:
            result = subprocess.run(
                [
                    'gcloud', 'firestore', 'export',
                    export_path,
                    '--collection-ids=users,departments,semesters,subjects,modules,notes'
                ],
                capture_output=True,
                text=True,
                check=True
            )
            
            logger.info("Backup completed successfully!")
            logger.info(f"Backup location: {export_path}")
            logger.info(result.stdout)
            
            return export_path
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Backup failed: {e}")
            logger.error(e.stderr)
            raise
        except FileNotFoundError:
            logger.error("gcloud command not found. Please install Google Cloud SDK.")
            raise

    def main():
        parser = argparse.ArgumentParser(description='Backup Firestore data')
        parser.add_argument(
            '--bucket',
            help='Cloud Storage bucket for backup (gs://bucket-name)'
        )
        
        args = parser.parse_args()
        
        try:
            backup_path = create_backup(args.bucket)
            print(f"\nBackup created: {backup_path}")
            print("\nTo restore from this backup:")
            print(f"gcloud firestore import {backup_path}")
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            exit(1)

    if __name__ == '__main__':
        main()
    ```
    
    This requires:
    1. Google Cloud SDK installed
    2. A Cloud Storage bucket created
    3. Proper IAM permissions
    
    Add documentation about prerequisites in the script header.
  </action>
  <verify>
    1. Check tools/backup_firestore.py exists
    2. Verify it uses gcloud firestore export
    3. Verify it generates timestamped backup paths
  </verify>
  <done>Backup script created for point-in-time recovery before migration</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] mock_db.json structure analyzed and documented
- [ ] tools/migration_config.py with field mappings and settings
- [ ] tools/seed_firestore.py with BulkWriter implementation
- [ ] Idempotency checks (schema version, _migrated_at) implemented
- [ ] Migration state tracking in _migrations collection
- [ ] tools/backup_firestore.py for pre-migration backups
- [ ] Error handling and retry logic in place
- [ ] Progress logging for visibility
- [ ] Command-line arguments (--dry-run, --collection, --reset) supported
- [ ] Script can be tested with dry-run before actual execution
</verification>

<success_criteria>
- All tasks completed
- Migration script is idempotent (can run multiple times safely)
- Uses BulkWriter for efficient batch operations
- Comprehensive logging and error handling
- State tracking for audit trail
- Backup script for safety
- Dry-run mode for testing
- Ready for 03-02 (Execute and Verify)
</success_criteria>

<output>
After completion, create `.planning/firebase-rbac-migration/phases/03-data-migration/03-01-SUMMARY.md`
</output>
